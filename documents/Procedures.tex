\documentclass{scrartcl}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[section]{minted}
\usemintedstyle[haskell]{lovelace}
\usepackage{amsthm}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}[defn]{Proposition}
\newtheorem{lemma}[defn]{Lemma}
\newtheorem{theo}[defn]{Theorem}
\newtheorem{exam}[defn]{Example}
\newtheorem{impl}[defn]{Code-Example}
\newtheorem{exer}[defn]{Exercise}
\usepackage[backend=biber,style=verbose-trad1]{biblatex}
\addbibresource{sicp.bib}
\numberwithin{equation}{section}

\begin{document}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\newminted{haskell}{bgcolor=bg, gobble=2, mathescape}
\title{Learn Functional Programming with Haskell}
\author{Oliver Krischer}
\maketitle
\begin{abstract}
  Working through the classical book \citetitle{sicp96} \autocite{sicp96} using \emph{Haskell}.
\end{abstract}
\tableofcontents
\listoffigures

\section{Building Abstractions with Procedures}

\begin{haskellcode}

> module Procedures where
> import Control.Monad ( replicateM_ )
> import Control.Monad.ST ( runST, ST )
> import Data.STRef ( newSTRef, readSTRef, writeSTRef )

\end{haskellcode}

\subsection{The Elements of Programming}
\subsubsection{Procedures as Black-Box Abstractions}

In the following example we include definitions for subroutines inside the main function \texttt{sqrtHeron}, in order to keep the user interface clean.
We also use \emph{lexical scoping}: all references to the input value \texttt{x} inside the subroutines receive their value directly from the main argument.

\begin{impl}
Implementing a squareroot function using the aproximation algorithm found by Heron of Alexandria.
\end{impl}

\begin{haskellcode}

> sqrtHeron :: Double -> Double
> sqrtHeron x = iter 1
>   where 
>     satisfies guess = abs (guess^2 - x) < 0.001
>     improve guess = (guess + (x / guess)) / 2
>     iter guess =
>       if satisfies guess then guess
>       else iter $ improve guess

\end{haskellcode}

\subsection{Procedures and the Processes they Generate}
\subsubsection{Linear Recursion and Iteration}

The \emph{factorial function} is defined by the following \emph{recurrence relation}:
\begin{align*}
    1! &= 1 \\ n! &= n \cdot (n-1)!
\end{align*}

\begin{impl}
A recursive definition of factorial, resulting in a recursive process.
\end{impl}

\begin{haskellcode}

> facRec :: Integer -> Integer
> facRec n = case n of
>   1 -> 1
>   n -> n * facRec (n-1)

\end{haskellcode}

\begin{impl}
In order to achieve an iterative process, we use the concept of accumulation for the following definition. For that, we `store' the running product as a parameter of the iteration function.
\end{impl}

\begin{haskellcode}

> facIter :: Integer -> Integer
> facIter = iter 1
>   where
>     iter p n = case n of
>       1 -> p
>       n -> iter (p*n) (n-1)

\end{haskellcode}

Observe, that we have a \emph{recursive function definition} in \textbf{both cases}, as this is the standard way of \emph{looping} in a pure functional programming language.
But the resulting \emph{computing process} differs in each case:
\begin{description}
\item[recursive:]
The program needs to keep track of the operations to be performed later on. Hence it has to store every frame of execution within the programs \emph{call stack}, which will grow and shrink during execution. This will lead to a space complexity of $\mathcal{O}(n)$ for this linear process.
\item[iterative:]
The program keeps track of the process with a fixed number of \emph{values} (in our case texttt{p} for the running product and \texttt{n} for decreasing the input value), which we repeatedly recalculate. The stack size keeps constant, resulting in a space complexity of $\mathcal{O}(1)$.
\end{description}
Another way of understanding this, is to look at the actual recursive call of the function:
if there are no additional calculations to be performed (i.e. the resulting value is immediately returned), the call will lead to an iterative process, provided the compiler is able to recognize and optimize this kind of \emph{tail recursive} calls.

\subsubsection{Tree Recursion}

Let's have a look at another standard example for recurrence relations, the \emph{Fibonacci numbers}:
\begin{align*}
    Fib_0 &= 0 \\ Fib_1 &= 1 \\ Fib_n &= Fib_{n-1} + Fib_{n-2}
\end{align*}

\begin{impl}
Recursive definition of Fibonacci numbers.
\end{impl}

\begin{haskellcode}

> fibRec :: Integer -> Integer
> fibRec n
>   | n < 2 = n
>   | otherwise = fibRec (n-1) + fibRec (n-2)

\end{haskellcode}

With that recursion expression we are calling the function twice for every $n>1$, which leads to an exponential growth of calculation steps.
The resulting process looks like a tree, in which the branches split into two at each level.
In general, the number of steps required by a \emph{tree-recursive} process will be proportional to the number of nodes in the tree, while the space required will be proportional to the maximum depth of the tree.

\begin{impl}
Iterative definition of Fibonacci numbers, using accumulation for the current sum.
\end{impl}

\begin{haskellcode}

> fibIter :: Integer -> Integer
> fibIter = iter 0 1
>   where
>     iter a b n = case n of
>       0 -> a
>       n -> iter b (a+b) (n-1)

\end{haskellcode}

\includegraphics[width=0.9\textwidth]{../img/fib_rec.png}

Benchmarking both implementations for $n=20$, we can see that the iterative function is much faster than the recursive one. This is mainly due to the fact that we have reduced the tree recursion to a linear one.

\begin{impl}
Going one step further, we can implement fibonacci numbers in an imperative style. For that, we use the concept of monads, which allows working with mutable variables that are local to the enclosing monad.
\end{impl}

An imperative version of \texttt{fibonacci} in C would look like this:
\begin{minted}[bgcolor=bg]{c}
int fibonacci(int n) {
   int a = 0;
   int b = 1;
   int t = 0;
   for (int i = 0; i < n; i++) {
      t = b;
      b += a;
      a = t;
   }
   return a;
}
\end{minted}

We can translate the C code almost directly into Haskell\footnote{this implementation is adopted from chapter 10 of \parencite{tfwh15}}, thereby getting rid of the temporary variable \texttt{t}:

\begin{haskellcode*}{linenos=true}

> fibST :: Int -> ST s Integer
> -- returns a monad of type Control.Monad.ST

> fibST n = do
>   a <- newSTRef 0
>   b <- newSTRef 1
>   replicateM_ n (do 
>     x <- readSTRef a
>     y <- readSTRef b
>     writeSTRef a y
>     writeSTRef b $! (x+y))
>   readSTRef a

\end{haskellcode*}

Here we have replaced the {\mintinline{c}{for}}-loop from the C program with a call to {\mintinline{haskell}{replicateM_}} from {\mintinline{haskell}{Control.Monad}} (lines 7 to 11).
We can now call that imperative code from a regular function:

\begin{haskellcode}

> fibImper :: Int -> Integer
> fibImper n = runST (fibST n)

\end{haskellcode}

\includegraphics[width=0.9\textwidth]{../img/fib_iter.png}

This version of \texttt{fibonacci} runs in constant space. In the result, for $n=100$ the imperative version is about twice as fast as the iterative one. This also tells us that the iterative version (which still is a recursive definition) gets optimized quite well by the Haskell compiler.

\subsubsection{Orders of Growth}

\subsubsection{Exponentiation}

\printbibliography
\end{document}